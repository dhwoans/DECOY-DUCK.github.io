---
layout: post
title: 캐시 메모리
subtitle: 속도차를 극복하는 방법
banner:
  image: https://t4.ftcdn.net/jpg/05/08/33/17/360_F_508331797_iOmrVTa5Bna98I5JZPiGvrczEvgX437J.jpg
  opacity: 0.618
  background: "#000"
  heading_style: "font-size: 4.25em; font-weight: bold; "
author: 오재문
categories: computer architecture
tags: web,DNS
comments: true

---

![image](https://user-images.githubusercontent.com/51963264/195114241-44f9d1c5-4504-41c0-912e-b1115e6c4cd5.png)

CPU의 처리속도와 메모리의 엑세스 속도의 격차를 극복하지 못하면 CPU 처리 속도가 아무리 뛰어나더라도 제대로된 성능을 내지 못합니다. 이런 속도 차이를 극복하기 위해 CPU가 앞으로 사용될 것이라 예상되는 데이터를 임시 저장소에 저장하게 되는데 이 임시 저장소를 `캐시 cache`라고 합니다.

![image](https://user-images.githubusercontent.com/51963264/226170769-9fb2a579-5211-4634-bb16-6265ee086a89.png)

캐시 메모리는 컴퓨터 시스템의 프로세서와 주 메모리(RAM) 사이에 있는 작고 빠른 휘발성 저장 장치입니다. 주요 목적은 자주 사용하는 데이터나 명령을 임시로 저장하여 프로세서가 액세스하는 데 걸리는 시간을 줄여 시스템의 전반적인 성능과 효율성을 향상시키는 것입니다.그래서 일반적으로 메인 메모리에 사용되는 DRAM(Dynamic Random Access Memory)보다 빠르고 비용이 많이 드는 SRAM(Static Random Access Memory) 기술을 사용합니다. 최신 컴퓨터 시스템은 성능, 비용 등을 고려하여 다단계 캐시 계층 구조를 구현합니다.

## 캐시 성능

캐시 히트가 되는 비율을 `캐시 적중률`이라고 하는데 결국 캐시 성능은 이 캐시 적중률에 달려있습니다. 

캐시 적중률을 높이는 방법에는 단순히 용량을 늘리는 것이 있습니다. 용량이 커지면 더 많은 데이터를 미리 가져올 수 있습니다. 하지만 용량을 늘린다는 것은 물리적 크기가 커진다는 것을 의미하고 이는 곧 비용이 증가한다는 뜻도 되기 떄문에 용량을 늘릴때 공간적 비용적인 제약을 고려해야 해야 합니다.

캐시 적중률을 높이는 또 다른 방법은 앞으로 **많이 사용될 데이터를 예측**하는 것이다. 이것은 연구를 통해서 메모리를 참조할 때 한정된 영역에서 이루어 지는 경향이 있다는 것을 발견했고 이러한 현상을 지역성 원리로 설명하고 있습니다. 지역성 원리는 공간 지역성과 시간 지역성으로 나눠져 있습니다.

`공간 지역성`

기억 장치 내 서로 인접하게 저장되어 있는 데이터는 연속적으로 엑세스될 확률이  높아지는 특성.

`시간 지역성`

최근에 엑세스된 데이터가 다시 엑세스될 확률이 높아지는 특성

---

캐시 메모리를 설계하면서 몇가지 고려사항이 있습니다. 주기억장치의 데이터를 캐시 메모리에서 어떤 방식으로 가져올것인가, 캐시메모리가 다 찼을때 어떻게 할것인가, 케시에 있는 데이터가 변경될때 어떻게 해야할 것인가 등이 이에 해당합니다. 

## 캐시 매핑기법

![image](https://user-images.githubusercontent.com/51963264/199429726-c3c8d173-0ef4-4745-9e6c-21e6f774e57e.png)

CPU가 캐시 메모리의 데이터를 읽으려고할 때 캐시히트와 캐시미스라는 두가지 상황을 생각할 수 있을 것 입니다. 캐시 미스가 발생할 경우 주기억장치의 데이터를 캐시메모리로 가져오게 되는데 이때 다양한 매핑기술이 사용이 됩니다.

구현이 간단한 것부터 살펴보겠습니다.바로 `직접 매핑 Direct Mapping`입니다. 




이렇기 때문에, 직접 사상은 교체 방식을 신경쓸 필요가 없어서,구현이 단순하고, 접근속도가 빠릅니다. 하지만 매번 교체를 하기 때문에, 동일 캐시 블록에 매핑되는 다른 메모리가 번갈아 실행되면 매우 낮은 적중률을 보여준다.
캐시 충돌을 완화하려면 캐시 크기를 늘리거나, 연관 매핑과 같은 다른 캐시 매핑 기술을 사용하여 캐시의 유연성을 높일 수 있습니다. 연관 매핑 기술에는 `완전 연관 매핑`과 `집합 연관 매핑`이 있습니다.

2. 완전 연관 매핑: 이 기술에서는 모든 메모리 블록을 모든 캐시 라인에 배치할 수 있습니다. 캐시 컨트롤러는 요청된 메모리 블록에 대한 모든 캐시 라인을 검색하고 새 블록을 로드해야 하는 경우 기존 블록을 대체할 수 있습니다. 이 기술은 캐시 충돌을 줄여주지만 캐시를 검색하고 관리하려면 더 복잡한 하드웨어가 필요합니다.

3. 집합 연관 매핑: 이 기술은 직접 매핑된 캐싱과 완전 연관 캐싱 간의 절충안입니다. 캐시는 여러 세트로 나뉘며 각 세트에는 여러 캐시 라인이 포함됩니다. 메모리 블록은 지정된 세트 내의 모든 캐시 라인에 배치할 수 있습니다. 이 접근 방식은 하드웨어 복잡성을 상대적으로 낮게 유지하면서 캐시 충돌을 줄입니다.

---

## 교체 정책

캐시 교체 정책은 캐시가 가득 찼을 때 새 데이터를 위한 공간을 만들기 위해 제거해야 하는 캐시 블록을 결정합니다. 이러한 정책의 목표는 캐시미스를 최소화하면서 가장 유용한 데이터를 캐시에 유지하여 캐시 성능을 최대화하는 것입니다. 다음은 대표적인 캐시 교체 정책입니다.

- LRU(Least Recently Used)    
 LRU 정책은 가장 최근에 액세스한 캐시 블록을 제거합니다. 이 정책은 블록에 한동안 액세스하지 않은 경우 가까운 장래에 액세스할 가능성이 적다는 가정을 기반으로 합니다. LRU는 연결 목록 또는 LRU 스택과 같은 특수 데이터 구조를 사용하여 구현할 수 있는 액세스 순서를 추적하기 위해 추가 기록이 필요합니다.

- FIFO(First-In, First-Out)   
 FIFO 정책은 캐시에 가장 먼저 로드된 가장 오래된 캐시 블록을 제거합니다. 이 정책은 구현이 간단하지만 데이터의 실제 사용 패턴을 고려하지 않기 때문에 항상 최상의 성능을 제공하는 것은 아닙니다.

- LFU(Least Frequency Used)   
 LFU 정책은 자주 액세스하지 않는 데이터가 곧 다시 필요할 가능성이 적다는 가정하에 가장 적게 액세스된 캐시 블록을 제거합니다. 이 정책은 액세스 빈도를 추적하기 위해 추가 기록이 필요하며 다른 정책보다 구현하기가 더 복잡할 수 있습니다.

각각의 캐시 교체 정책에는 강점과 약점이 있으며 특정 애플리케이션에 대한 최상의 정책은 요구 사항에 따라 다릅니다.

---

## 쓰기 정책

쓰기 정책은 프로세서가 메모리 블록을 수정할 때 캐시와 메인 메모리가 업데이트되는 방식을 결정합니다. 쓰기 정책의 선택은 시스템의 성능, 복잡성 및 일관성에 상당한 영향을 미칠 수 있습니다. 다음은 몇 가지 추가 변형 및 고려 사항과 함께 두 가지 기본 쓰기 정책에 대한 자세한 설명입니다.

- 즉시 쓰기 Write-Through   
  write-through 정책에서 프로세서는 데이터를 캐시와 주 메모리 모두에 동시에 씁니다. 이 접근 방식은 주 메모리에 항상 최신 데이터가 포함되도록 하여 더 나은 일관성을 제공하고 정전이나 시스템 충돌 시 데이터 손실 위험을 줄입니다.

  그러나 write-through는 모든 쓰기 작업이 캐시와 메인 메모리를 모두 업데이트해야 하므로 메모리 트래픽과 대기 시간이 증가할 수 있습니다. 또한 이 정책은 특히 쓰기 집약적인 워크로드를 처리할 때 캐시 대역폭을 비효율적으로 사용할 수 있습니다.

- 지연 쓰기 write back  
  쓰기 되돌림 정책에서 프로세서는 데이터를 캐시에만 씁니다. 수정된 캐시 블록은 "더티"로 표시되어 해당 내용이 해당 주 메모리 블록과 다르다는 것을 나타냅니다. 더티 블록이 캐시에서 제거된 경우에만 데이터가 주 메모리에 다시 기록됩니다.

  후기입은 프로세서가 느린 주 메모리 업데이트를 기다리지 않고 명령을 계속 실행할 수 있으므로 메모리 트래픽을 줄이고 전반적인 시스템 성능을 향상시킵니다. 그러나 이 정책에는 특히 다중 코어 또는 다중 프로세서 시스템에서 캐시 일관성 및 일관성을 유지하기 위한 추가 논리가 필요합니다.

  또한 쓰기 되돌림 정책은 수정된 데이터가 주 메모리가 아닌 캐시에만 상주할 수 있으므로 정전이나 시스템 충돌 시 데이터 손실 위험을 높입니다. 이러한 위험을 완화하기 위해 일부 시스템에서는 배터리 백업 또는 비휘발성 캐시를 사용합니다. 

---

## 멀티프로세서에서의 캐시 공유

멀티프로세서 시스템에서는 여러 프로세서가 동시에 메모리에 액세스하므로, 캐시 메모리와 관련하여 추가적인 문제들이 발생할 수 있습니다. 주요 문제들은 다음과 같습니다.

- 캐시 일관성 (Cache Coherence)    
멀티프로세서 시스템에서는 각 프로세서가 자체 캐시를 가지고 있을 수 있습니다. 이로 인해 한 프로세서가 메모리 위치를 업데이트했을 때, 다른 프로세서의 캐시에서 여전히 이전 값을 가지고 있을 수 있습니다. 이를 캐시 일관성 문제라고 하며, 이로 인해 프로그램의 동작이 올바르지 않을 수 있습니다. 캐시 일관성 문제를 해결하기 위해 여러 가지 프로토콜과 메커니즘이 사용됩니다. 예를 들어, 스누프(Snooping)와 디렉토리 기반(Directory-based) 프로토콜이 있습니다.

- 메모리 동기화 Memory Synchronization    
멀티프로세서 시스템에서 프로세서 간의 메모리 동기화는 중요한 문제입니다. 프로세서들이 공유 데이터에 액세스하거나 수정할 때, 일관된 순서를 유지해야 합니다. 이를 위해 메모리 배리어(Memory Barrier)와 같은 동기화 기법이 사용됩니다.

- 데드락 Deadlock    
멀티프로세서 시스템에서 두 개 이상의 프로세서가 동일한 자원에 동시에 액세스하려고 할 때, 서로가 다른 프로세서가 자원을 해제하기를 기다리는 상황이 발생할 수 있습니다. 이런 상황을 데드락이라고 하며, 시스템의 전체 성능에 심각한 영향을 미칠 수 있습니다. 데드락을 방지하거나 해결하기 위한 여러 가지 방법이 있습니다.

- 캐시 쓰레싱 Cache Thrashing     
멀티프로세서 시스템에서 공유 캐시를 사용할 경우, 여러 프로세서가 동시에 캐시에 액세스하려고 할 때 캐시 스래싱이 발생할 수 있습니다. 캐시 스래싱은 캐시의 공간이 여러 프로세서 사이에서 경쟁적으로 사용되어 성능 저하를 초래하는 현상
